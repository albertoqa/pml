{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis - Housing Boston Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jun  8 12:38:15 2013\n",
    "@author: alberto\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import datasets, ensemble, metrics, preprocessing\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import tree, utils\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "def normalizacion():\n",
    "    scaler = preprocessing.StandardScaler().fit(boston_X)\n",
    "    scaler.mean_\n",
    "    scaler.std_\n",
    "    scaler.transform(boston_X) \n",
    "#FIN FUNCION normalizacion\n",
    "\n",
    "def aprendizajePorCaractGradient(caract):\n",
    "\n",
    "    clf = ensemble.GradientBoostingRegressor(n_estimators=350, max_depth=2, learning_rate=0.1, loss='ls', subsample=0.5)\n",
    "\n",
    "    #VALIDACION CRUZADA\n",
    "    mae=mse=r2=0\n",
    "    kf = KFold(len(boston_Y), n_folds=10, indices=True)\n",
    "    for train, test in kf:\n",
    "        trainX, testX, trainY, testY=boston_X[train], boston_X[test], boston_Y[train], boston_Y[test]\n",
    "        \n",
    "        nCar=len(caract)\n",
    "        train=np.zeros((len(trainX), nCar))\n",
    "        test=np.zeros((len(testX), nCar))\n",
    "        trainYNuevo=trainY\n",
    "        \n",
    "        for i in range(nCar):\n",
    "            for j in range(len(trainX)):\n",
    "                train[j][i]=trainX[j][caract[i]]\n",
    "                \n",
    "            for k in range(len(testX)):\n",
    "                test[k][i]=testX[k][caract[i]]\n",
    "        \n",
    "        trainYNuevo=np.reshape(trainYNuevo, (len(trainY), -1))\n",
    "        \n",
    "        clf.fit(train, trainYNuevo)\n",
    "        prediccion=clf.predict(test)\n",
    "        \n",
    "        mae+=metrics.mean_absolute_error(testY, prediccion)\n",
    "        mse+=metrics.mean_squared_error(testY, prediccion)\n",
    "        r2+=metrics.r2_score(testY, prediccion)\n",
    "    \n",
    "    #print str('\\nAprendizaje realizado con los atributos: ')+str(caract)\n",
    "    #print 'Error abs: ', mae/len(kf), 'Error cuadratico: ', mse/len(kf), 'R cuadrado: ', r2/len(kf)\n",
    "\n",
    "#FIN FUNCION aprendizajePorCaract\n",
    "\n",
    "def gradientBoosting():\n",
    "    \n",
    "    num_estimadores = 350\n",
    "    clf = ensemble.GradientBoostingRegressor(n_estimators=num_estimadores, max_depth=2, learning_rate=0.1, loss='ls', subsample=0.5)\n",
    "    \n",
    "    importancias = [0,0,0,0,0,0,0,0,0,0,0,0,0]    \n",
    "    mae, mse, mr2, cont = 0, 0, 0, 0\n",
    "    test_score = np.zeros((num_estimadores,), dtype=np.float64)\n",
    "    train_score = np.zeros((num_estimadores,), dtype=np.float64)\n",
    "    mseVector = [0]\n",
    "\n",
    "    kf = KFold(len(boston_Y), n_folds=10, indices=True)\n",
    "    for train, test in kf:\n",
    "        trainX, testX, trainY, testY=boston_X[train], boston_X[test], boston_Y[train], boston_Y[test]    \n",
    "    \n",
    "        clf.fit(trainX, trainY)\n",
    "        pred = clf.predict(testX)\n",
    "        \n",
    "        maeGradient = metrics.mean_absolute_error(testY, pred)\n",
    "        mseGradient = metrics.mean_squared_error(testY, pred)\n",
    "        r2 = metrics.r2_score(testY, pred)\n",
    "        \n",
    "        mae = mae + maeGradient\n",
    "        mse = mse + mseGradient     \n",
    "        mr2 = mr2 + r2\n",
    "        mseVector.append(mseGradient)\n",
    "        cont = cont + 1\n",
    "                \n",
    "        for i, y_pred in enumerate(clf.staged_decision_function(testX)):\n",
    "            test_score[i] = test_score[i] + clf.loss_(testY, y_pred)\n",
    "         \n",
    "        for i in range(num_estimadores):\n",
    "            train_score[i] = clf.train_score_[i] + train_score[i]\n",
    "    \n",
    "        feature_importance = clf.feature_importances_\n",
    "        feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "        for i in range(13):\n",
    "            importancias[i] = importancias[i] + feature_importance[i]\n",
    "\n",
    "        #print str(\"Iteracci√≥n \")+str(cont)+str(\" de la validacion cruzada\")\n",
    "        #print str(\"\\tError medio absoluto:  \")+str(maeGradient)\n",
    "        #print str(\"\\tError medio cuadrado:  \")+str(mseGradient)\n",
    "        #print str(\"\\tr2:  \")+str(r2)\n",
    "\n",
    "        #Dibuja los puntos que predice sobre los puntos verdaderos\n",
    "        pl.plot(testY, testY, label='Valor verdadero')\n",
    "        pl.plot(testY, pred, 'ro', label='Prediccion Gradient')\n",
    "        pl.legend(bbox_to_anchor=(1.05, 1), borderaxespad=0., prop = FontProperties(size='smaller'))\n",
    "        pl.show()\n",
    "\n",
    "    #print mseVector\n",
    "    mae = mae/10\n",
    "    mse = mse/10\n",
    "    mr2 = mr2/10\n",
    "    #print str(\"Error medio absoluto: \")+str(mae)+str(\"\\tError medio cuadratico: \")+str(mse)+str(\"\\tR2: \")+str(mr2)    \n",
    "    \n",
    "    for i in range(13):\n",
    "        importancias[i] = importancias[i]/10\n",
    "        \n",
    "    sorted_idx = np.argsort(importancias)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    importancias = np.reshape(importancias, (len(importancias), -1))\n",
    "\n",
    "    boston = datasets.load_boston()\n",
    "    pl.barh(pos, importancias[sorted_idx], align='center')\n",
    "    pl.yticks(pos, boston.feature_names[sorted_idx])\n",
    "    pl.xlabel('Importancia relativa')\n",
    "    pl.show()\n",
    "    \n",
    "    for i in range(num_estimadores):\n",
    "        test_score[i] = test_score[i]/10\n",
    "        train_score[i] = train_score[i]/10\n",
    "        \n",
    "    pl.figure(figsize=(12, 6))\n",
    "    pl.subplot(1, 1, 1)\n",
    "    pl.title('Desviacion')\n",
    "    pl.plot(np.arange(num_estimadores) + 1, train_score, 'b-', label='Error en el conjunto de Training')\n",
    "    pl.plot(np.arange(num_estimadores) + 1, test_score, 'r-', label='Error en el conjunto de Test')\n",
    "    pl.legend(loc='upper right')\n",
    "    pl.xlabel('Iteracciones del Boosting (numero de arboles)')\n",
    "    pl.ylabel('Desviacion')\n",
    "    pl.show()\n",
    "    \n",
    "    #print len(mseVector)\n",
    "    #print len(np.arange(10))    \n",
    "    \n",
    "    pl.subplot(1, 1, 1)\n",
    "    pl.plot(np.arange(11), mseVector, 'b-')\n",
    "    pl.legend(loc='upper right')\n",
    "    pl.xlabel('Iteraccion de la validacion cruzada')\n",
    "    pl.ylabel('Erro Medio Cuadratico')\n",
    "    pl.show()\n",
    "\n",
    "    \n",
    "    fig, axs = plot_partial_dependence(clf, trainX,[0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "    \n",
    "    fig.suptitle('Dependencia parcial del valor de las casas')\n",
    "    \n",
    "    pl.subplots_adjust(top=0.9)\n",
    "        \n",
    "    pl.show()\n",
    "#FIN FUNCION gradientBoosting\n",
    "\n",
    "def arbolesRegresion(caract):\n",
    "    \n",
    "    clf = DecisionTreeRegressor(min_samples_leaf=10, min_samples_split=15, max_depth=13, compute_importances=True)\n",
    "    \n",
    "    importancias = [0,0,0,0,0,0,0,0,0,0,0,0,0]    \n",
    "    mae=mse=r2=0\n",
    "    \n",
    "    kf = KFold(len(boston_Y), n_folds=10, indices=True)\n",
    "    for train, test in kf:\n",
    "        trainX, testX, trainY, testY=boston_X[train], boston_X[test], boston_Y[train], boston_Y[test]\n",
    "            \n",
    "        nCar=len(caract)\n",
    "        train=np.zeros((len(trainX), nCar))\n",
    "        test=np.zeros((len(testX), nCar))\n",
    "        trainYNuevo=trainY\n",
    "        \n",
    "        for i in range(nCar):\n",
    "            for j in range(len(trainX)):\n",
    "                train[j][i]=trainX[j][caract[i]]\n",
    "                \n",
    "            for k in range(len(testX)):\n",
    "                test[k][i]=testX[k][caract[i]]\n",
    "        \n",
    "        trainYNuevo=np.reshape(trainYNuevo, (len(trainY), -1))\n",
    "        \n",
    "        clf.fit(train, trainYNuevo)\n",
    "        prediccion=clf.predict(test)            \n",
    "        \n",
    "#        clf.fit(trainX, trainY)\n",
    "#        prediccion=clf.predict(testX)\n",
    "            \n",
    "        mae+=metrics.mean_absolute_error(testY, prediccion)\n",
    "        mse+=metrics.mean_squared_error(testY, prediccion)\n",
    "        r2+=metrics.r2_score(testY, prediccion)\n",
    "        \n",
    "        feature_importance = clf.feature_importances_\n",
    "        feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "        for i in range(13):\n",
    "            importancias[i] = importancias[i] + feature_importance[i]\n",
    "        \n",
    "    #print 'Error abs: ', mae/len(kf), 'Error cuadratico: ', mse/len(kf), 'R cuadrado: ', r2/len(kf)\n",
    "    \n",
    "    for i in range(13):\n",
    "        importancias[i] = importancias[i]/10\n",
    "        \n",
    "    sorted_idx = np.argsort(importancias)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    importancias = np.reshape(importancias, (len(importancias), -1))\n",
    "\n",
    "    boston = datasets.load_boston()\n",
    "    pl.barh(pos, importancias[sorted_idx], align='center')\n",
    "    pl.yticks(pos, boston.feature_names[sorted_idx])\n",
    "    pl.xlabel('Importancia relativa')\n",
    "    pl.show()    \n",
    "    \n",
    "    import StringIO, pydot \n",
    "    dot_data = StringIO.StringIO() \n",
    "    tree.export_graphviz(clf, out_file=dot_data) \n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "    graph.write_pdf(\"bostonTree.pdf\") \n",
    "\n",
    "\n",
    "def pruebaModelos():\n",
    "\n",
    "    gra = ensemble.GradientBoostingRegressor(n_estimators=350, max_depth=2, learning_rate=0.1, loss='ls', subsample=0.5)\n",
    "    svr = SVR(kernel='linear', C=0.1, epsilon=0.2)\n",
    "    reg = LinearRegression()\n",
    "    dtr = DecisionTreeRegressor(min_samples_leaf=10, min_samples_split=15, max_depth=13, compute_importances=True)\n",
    "    \n",
    "    cla=(gra, svr, reg, dtr)\n",
    "    \n",
    "    #print \"Validazion cruzada para los 5 modelos(en orden: GradientBoosting, SVR, LinearRegression, TreeRegressor)\"\n",
    "    for c in cla:\n",
    "        #VALIDACION CRUZADA\n",
    "        mae=mse=r2=0\n",
    "        kf = KFold(len(boston_Y), n_folds=10, indices=True)\n",
    "        for train, test in kf:\n",
    "            trainX, testX, trainY, testY=boston_X[train], boston_X[test], boston_Y[train], boston_Y[test]\n",
    "            \n",
    "            c.fit(trainX, trainY)\n",
    "            prediccion=c.predict(testX)\n",
    "            \n",
    "            mae+=metrics.mean_absolute_error(testY, prediccion)\n",
    "            mse+=metrics.mean_squared_error(testY, prediccion)\n",
    "            r2+=metrics.r2_score(testY, prediccion)\n",
    "        #print 'Error abs: ', mae/len(kf), 'Error cuadratico: ', mse/len(kf), 'R cuadrado: ', r2/len(kf)\n",
    "        #FIN FOR VALIDACION CRUZADA\n",
    "    #FIN FOR CLASIFICADORES\n",
    "#FIN pruebaModelos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tecnicaSVR():\n",
    "    \n",
    "    parametros = [{'kernel':'linear', 'C':0.1, 'epsilon':0.2},\n",
    "                  {'kernel':'linear', 'C':1.0, 'epsilon':0.2},\n",
    "                  {'kernel':'rbf', 'degree':3, 'gamma':.0001, 'C':1.0, 'epsilon':0.2},\n",
    "                  {'kernel':'rbf', 'degree':2, 'gamma':.01, 'C':0.1, 'epsilon':0.2}]\n",
    "\n",
    "    mae=mse=r2=0\n",
    "    \n",
    "    for c in parametros:\n",
    "        clf = SVR(**c)\n",
    "        #VALIDACION CRUZADA\n",
    "        mae=mse=r2=0\n",
    "        kf = KFold(len(boston_Y), n_folds=10, indices=True)\n",
    "        for train, test in kf:\n",
    "            trainX, testX, trainY, testY=boston_X[train], boston_X[test], boston_Y[train], boston_Y[test]\n",
    "            \n",
    "            clf.fit(trainX, trainY)\n",
    "            prediccion=clf.predict(testX)\n",
    "            \n",
    "            mae+=metrics.mean_absolute_error(testY, prediccion)\n",
    "            mse+=metrics.mean_squared_error(testY, prediccion)\n",
    "            r2+=metrics.r2_score(testY, prediccion)\n",
    "        \n",
    "            #print clf.coef_\n",
    "        #print \"Parametros: \", c\n",
    "        #print 'Error abs: ', mae/len(kf), 'Error cuadratico: ', mse/len(kf), 'R cuadrado: ', r2/len(kf)\n",
    "        mae=mse=r2=0\n",
    "\n",
    "\n",
    "\n",
    "def ajustesParametrosG():\n",
    "    parametros_gradient = [\n",
    "                            #{'loss':'ls'},{'loss':'lad'}, {'loss':'huber'},\n",
    "                           #{'n_estimators': 500, 'loss':'ls'},\n",
    "                           {'n_estimators': 350, 'max_depth':2, 'learning_rate': 0.1, 'loss': 'ls', 'subsample':0.5},\n",
    "                           {'n_estimators': 350, 'max_depth':2, 'learning_rate': 0.1, 'loss': 'ls'},\n",
    "                           {'n_estimators': 500, 'max_depth': 2, 'min_samples_split': 4, 'min_samples_leaf':1, 'learning_rate': 0.01, 'loss': 'ls'},\n",
    "                           {'n_estimators': 500, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf':1, 'learning_rate': 0.1, 'loss': 'ls'}]\n",
    "\n",
    "\n",
    "    num_estimadores = 350\n",
    "    test_score = np.zeros((num_estimadores,), dtype=np.float64)\n",
    "    train_score = np.zeros((num_estimadores,), dtype=np.float64)\n",
    "\n",
    "    for c in parametros_gradient:\n",
    "        #print c\n",
    "        clf = ensemble.GradientBoostingRegressor(**c)\n",
    "        #VALIDACION CRUZADA\n",
    "        mae=mse=r2=0\n",
    "        kf = KFold(len(boston_Y), n_folds=10, indices=True)\n",
    "        for train, test in kf:\n",
    "            trainX, testX, trainY, testY=boston_X[train], boston_X[test], boston_Y[train], boston_Y[test]\n",
    "            \n",
    "            clf.fit(trainX, trainY)\n",
    "            prediccion=clf.predict(testX)\n",
    "            \n",
    "            mae+=metrics.mean_absolute_error(testY, prediccion)\n",
    "            mse+=metrics.mean_squared_error(testY, prediccion)\n",
    "            r2+=metrics.r2_score(testY, prediccion)\n",
    "            \n",
    "            for i, y_pred in enumerate(clf.staged_decision_function(testX)):\n",
    "                test_score[i] = test_score[i] + clf.loss_(testY, y_pred)\n",
    "         \n",
    "            for i in range(num_estimadores):\n",
    "                train_score[i] = clf.train_score_[i] + train_score[i]            \n",
    "            \n",
    "            \n",
    "        #print 'Error abs: ', mae/len(kf), 'Error cuadratico: ', mse/len(kf), 'R cuadrado: ', r2/len(kf)\n",
    "\n",
    "        for i in range(num_estimadores):\n",
    "            test_score[i] = test_score[i]/10\n",
    "            train_score[i] = train_score[i]/10\n",
    "        \n",
    "        pl.figure(figsize=(12, 6))\n",
    "        pl.subplot(1, 1, 1)\n",
    "        pl.title('Desviacion')\n",
    "        pl.plot(np.arange(num_estimadores) + 1, train_score, 'b-', label='Error en el conjunto de Training')\n",
    "        pl.plot(np.arange(num_estimadores) + 1, test_score, 'r-', label='Error en el conjunto de Test')\n",
    "        pl.legend(loc='upper right')\n",
    "        pl.xlabel('Iteracciones del Boosting (numero de arboles)')\n",
    "        pl.ylabel('Desviacion')\n",
    "        pl.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "#                                   #\n",
    "#   Llamadas a las funciones        #\n",
    "#                                   #\n",
    "#####################################\n",
    "\n",
    "boston=datasets.load_boston()\n",
    "boston_X= boston.data\n",
    "boston_Y= boston.target\n",
    "\n",
    "#normalizacion()\n",
    "#pruebaModelos()\n",
    "#aprendizajePorCaractGradient([0,2,3,4,5,6,7,8,9,10,11,12])\n",
    "#gradientBoosting()\n",
    "#arbolesRegresion([0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "#tecnicaSVR()\n",
    "#ajustesParametrosG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
